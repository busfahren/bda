{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy-Based Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "[A Simple Guide to Entropy-Based Discretization - Kevin Meurer](http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/)\n",
    "\n",
    "#### Calculation of entropy\n",
    "$Entropy(D) = -\\sum_{i = 1}^{m} p_i \\cdot \\log_2 p_i$\n",
    "\n",
    "Where $D$ is our data. $m$ the number of classifier values we have and $p_i$ the probability for each classifier.\n",
    "\n",
    "#### Calculation of net entropy for a split\n",
    "$NetEntropy(D) = \\cfrac{\\left\\vert{D_1}\\right\\vert}{\\left\\vert{D}\\right\\vert} \\cdot Entropy(D_1) + \\cfrac{\\left\\vert{D_2}\\right\\vert}{\\left\\vert{D}\\right\\vert} \\cdot Entropy(D_2)$\n",
    "\n",
    "The net entropy with splits is calculated by taking the entropy for each split and weighting it by it's share in the entire dataset.\n",
    "\n",
    "#### Calculation of entropy gain for a split\n",
    "$Gain(E_{new}) = E_{initial} - E_{new}$\n",
    "\n",
    "Where $E_{inital}$ is the initial entropy of the set and $E_{new}$  is the new entropy with split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "# Entropy calculator\n",
    "def entropy(data):\n",
    "    labels = [d[1] for d in data] # Get label for each data point\n",
    "    distinct_labels = set(labels) # Get distinct labels in set\n",
    "    \n",
    "     # If there is only one label return entropy of 1 ( to avoid log_2(1) )\n",
    "    if len(distinct_labels) == 1: \n",
    "        print(\"Total: %d\\n%s:\\n\\tcount: %d\\tprob: %.3f\\nEntropy: %.3f\" % \n",
    "              (len(data), distinct_labels.pop(), len(data), 1.0, 0.0))\n",
    "        return 0.0\n",
    "\n",
    "    probs = [ # Calculate probabilites for each distinct label\n",
    "        labels.count(label)/len(data) # Count of each label by total number of occurrences\n",
    "        for label in distinct_labels]\n",
    "    \n",
    "    # Some printing\n",
    "    print(\"Total: %d\" % len(data))\n",
    "    for label in distinct_labels:\n",
    "        count = labels.count(label)\n",
    "        prob = count/len(data)\n",
    "        bits = log(prob, 2)\n",
    "        print(\"%s:\\n\\tcount: %d\\tprob: %.3f\\tlog_2(p): %.3f\\tp*log:%.3f\" % \n",
    "              (label, count, prob, bits, prob*bits))\n",
    "    print(\"Entropy: %.3f\" % -sum([p * log(p, 2) for p in probs]))\n",
    "        \n",
    "    # Calculate total entropy by weighting the number of bits necessary to\n",
    "    # represent the probability ( log_2(probability) ) by the probability itself\n",
    "    return -sum([p * log(p, 2) for p in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 8\n",
      "FAIL:\n",
      "\tcount: 4\tprob: 0.500\tlog_2(p): -1.000\tp*log:-0.500\n",
      "OK:\n",
      "\tcount: 4\tprob: 0.500\tlog_2(p): -1.000\tp*log:-0.500\n",
      "Entropy: 1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(0.1,   'FAIL'),\n",
    "        (0.2,   'FAIL'),\n",
    "        (0.8,   'FAIL'),\n",
    "        (0.9,   'OK'),\n",
    "        (1.0,   'FAIL'),\n",
    "        (4.0,   'OK'),\n",
    "        (10.0,  'OK'),\n",
    "        (50.0,  'OK')]\n",
    "\n",
    "total_entropy = entropy(data)\n",
    "total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_gain(candidate_boundary):\n",
    "    bin_1 = [d for d in data if d[0] < candidate_boundary]\n",
    "    bin_2 = [d for d in data if d[0] > candidate_boundary]\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(\"Bin 1:\", bin_1)\n",
    "    entropy_1 = entropy(bin_1)\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(\"Bin 2:\", bin_2)\n",
    "    entropy_2 = entropy(bin_2)\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    total = len(data)\n",
    "    net_entropy = len(bin_1)/total * entropy_1 + len(bin_2)/total * entropy_2\n",
    "    print(\"New net entropy:\")\n",
    "    print(\"(%d/%d) * %.3f + (%d/%d) * %.3f = %.3f\" % \n",
    "          (len(bin_1), total, entropy_1, len(bin_2), total, entropy_2, net_entropy))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(\"Entropy gain:\")\n",
    "    print(\"%.3f = %.3f (initial) - %.3f (new)\" % (total_entropy - net_entropy, total_entropy, net_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Bin 1: [(0.1, 'FAIL'), (0.2, 'FAIL')]\n",
      "Total: 2\n",
      "FAIL:\n",
      "\tcount: 2\tprob: 1.000\n",
      "Entropy: 0.000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Bin 2: [(0.8, 'FAIL'), (0.9, 'OK'), (1.0, 'FAIL'), (4.0, 'OK'), (10.0, 'OK'), (50.0, 'OK')]\n",
      "Total: 6\n",
      "FAIL:\n",
      "\tcount: 2\tprob: 0.333\tlog_2(p): -1.585\tp*log:-0.528\n",
      "OK:\n",
      "\tcount: 4\tprob: 0.667\tlog_2(p): -0.585\tp*log:-0.390\n",
      "Entropy: 0.918\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "New net entropy:\n",
      "(2/8) * 0.000 + (6/8) * 0.918 = 0.689\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Entropy gain:\n",
      "0.311 = 1.000 (initial) - 0.689 (new)\n"
     ]
    }
   ],
   "source": [
    "entropy_gain(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Bin 1: [(0.1, 'FAIL'), (0.2, 'FAIL'), (0.8, 'FAIL'), (0.9, 'OK')]\n",
      "Total: 4\n",
      "FAIL:\n",
      "\tcount: 3\tprob: 0.750\tlog_2(p): -0.415\tp*log:-0.311\n",
      "OK:\n",
      "\tcount: 1\tprob: 0.250\tlog_2(p): -2.000\tp*log:-0.500\n",
      "Entropy: 0.811\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Bin 2: [(1.0, 'FAIL'), (4.0, 'OK'), (10.0, 'OK'), (50.0, 'OK')]\n",
      "Total: 4\n",
      "FAIL:\n",
      "\tcount: 1\tprob: 0.250\tlog_2(p): -2.000\tp*log:-0.500\n",
      "OK:\n",
      "\tcount: 3\tprob: 0.750\tlog_2(p): -0.415\tp*log:-0.311\n",
      "Entropy: 0.811\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "New net entropy:\n",
      "(4/8) * 0.811 + (4/8) * 0.811 = 0.811\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Entropy gain:\n",
      "0.189 = 1.000 (initial) - 0.811 (new)\n"
     ]
    }
   ],
   "source": [
    "entropy_gain(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Bin 1: [(0.1, 'FAIL'), (0.2, 'FAIL'), (0.8, 'FAIL'), (0.9, 'OK'), (1.0, 'FAIL')]\n",
      "Total: 5\n",
      "FAIL:\n",
      "\tcount: 4\tprob: 0.800\tlog_2(p): -0.322\tp*log:-0.258\n",
      "OK:\n",
      "\tcount: 1\tprob: 0.200\tlog_2(p): -2.322\tp*log:-0.464\n",
      "Entropy: 0.722\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Bin 2: [(4.0, 'OK'), (10.0, 'OK'), (50.0, 'OK')]\n",
      "Total: 3\n",
      "OK:\n",
      "\tcount: 3\tprob: 1.000\n",
      "Entropy: 0.000\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "New net entropy:\n",
      "(5/8) * 0.722 + (3/8) * 0.000 = 0.451\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Entropy gain:\n",
      "0.549 = 1.000 (initial) - 0.451 (new)\n"
     ]
    }
   ],
   "source": [
    "print_info(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
